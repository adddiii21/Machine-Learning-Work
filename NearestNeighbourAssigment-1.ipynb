{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74836aa2",
   "metadata": {},
   "source": [
    "# Task-1 Loading datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af6dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "ion = np.genfromtxt(\"ionosphere.txt\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b614e302",
   "metadata": {},
   "source": [
    "# Task-2.1- Splitting iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34882fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 4) (38, 4)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(iris['data'], iris['target'], random_state=2105)\n",
    "print(X_iris_train.shape, X_iris_test.shape)\n",
    "print(y_iris_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d910f25",
   "metadata": {},
   "source": [
    "# Task-2.2- Splitting ionosphere dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "455ad308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88, 34)\n"
     ]
    }
   ],
   "source": [
    "X_ion = np.genfromtxt(\"ionosphere.txt\", delimiter=\",\", usecols=np.arange(34))\n",
    "y_ion = np.genfromtxt(\"ionosphere.txt\", delimiter=\",\", usecols=34, dtype='int')\n",
    "X_ion_train, X_ion_test, y_ion_train, y_ion_test = train_test_split(X_ion, y_ion, random_state=2105)\n",
    "print(X_ion_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8de0a2",
   "metadata": {},
   "source": [
    "# Task 3.1- Nearest Neighbour method implementation\n",
    "\n",
    "Implementing the nearest neighbours method was relatively straightforward. It relied on the calculation of Euclidean distance between points so I created a seperate helper function to compute those distances. The main for loop iterates over every sample in the test set and sets an initial minimum distance to infinity and index of nearest neighbour to -1. The inner loop iterates over the samples in the training set and computes the distance between the test sample and the training sample. It compares one test sample with every training sample before moving to the next test sample(if there is more than one being passed to the function). Once it has the index of the training sample with the smallest distance to the test sample, it retrieves the corresponding label for that training sample and uses that as the classification for the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d296fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "['setosa']\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "    \n",
    "def nearest_neighbor(X_train, y_train, X_test):\n",
    "    #Initialise an empty list to store all nearest neighbours\n",
    "    predictions = []\n",
    "    for test_value in X_test:\n",
    "        min_distance = math.inf\n",
    "        nearest_index = -1\n",
    "        #Loop through all training samples and calculate distance to each test sample.\n",
    "        for i in range(X_train.shape[0]):\n",
    "            distance = euclidean_distance(test_value, X_train[i])\n",
    "            #If the distance is smaller than the current minimum, update it.\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                nearest_index = i\n",
    "        #Get the label of the training sample with the smallest distance to the test sample.\n",
    "        #Use that label as the prediction for the test sample.\n",
    "        predictions.append(y_train[nearest_index])\n",
    "    predictions_np = np.array(predictions)\n",
    "    return predictions_np\n",
    "\n",
    "#Checking that my implementation gives the same prediction as the sci-kit learn method\n",
    "new_iris = np.array([[5, 2.9, 1, 0.2]])\n",
    "prediction = nearest_neighbor(X_iris_train, y_iris_train, new_iris)\n",
    "print(prediction)\n",
    "print(iris['target_names'][prediction])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d6e67a",
   "metadata": {},
   "source": [
    "# Task 3.2 -Iris dataset error calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d7f3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n",
      "NUMBER OF ERRORS in IRIS dataset classification for K=1: 1\n",
      "IRIS dataset ERROR RATE for K=1: 0.02631578947368421\n"
     ]
    }
   ],
   "source": [
    "prediction_all_iris = nearest_neighbor(X_iris_train, y_iris_train, X_iris_test)\n",
    "print(np.mean(prediction_all_iris == y_iris_test))\n",
    "num_errors_iris = np.sum(prediction_all_iris != y_iris_test)\n",
    "print(\"NUMBER OF ERRORS in IRIS dataset classification for K=1:\", num_errors_iris)\n",
    "err_rate_iris = num_errors_iris/len(y_iris_test)\n",
    "print(\"IRIS dataset ERROR RATE for K=1:\", err_rate_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f3d10",
   "metadata": {},
   "source": [
    "# Task 3.2 -Ionosphere dataset error calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee0e4a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n",
      "NUMBER OF ERRORS in IONOSPHERE dataset classification for K=1: 8\n",
      "IONOSPHERE dataset ERROR RATE for K=1: 0.09090909090909091\n"
     ]
    }
   ],
   "source": [
    "prediction_all_ion = nearest_neighbor(X_ion_train, y_ion_train, X_ion_test)\n",
    "print(np.mean(prediction_all_ion == y_ion_test))\n",
    "num_errors_ion = np.sum(prediction_all_ion != y_ion_test)\n",
    "print(\"NUMBER OF ERRORS in IONOSPHERE dataset classification for K=1:\", num_errors_ion)\n",
    "err_rate_ion = num_errors_ion/len(y_ion_test)\n",
    "print(\"IONOSPHERE dataset ERROR RATE for K=1:\", err_rate_ion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35bf0e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 4)\n",
      "(37, 4)\n",
      "(36, 4)\n"
     ]
    }
   ],
   "source": [
    "label_zero = X_iris_train[y_iris_train == 0]\n",
    "print(label_zero.shape)\n",
    "label_one = X_iris_train[y_iris_train == 1]\n",
    "print(label_one.shape)\n",
    "label_two = X_iris_train[y_iris_train == 2]\n",
    "print(label_two.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d452121",
   "metadata": {},
   "source": [
    "# Conformal Predictor Implementation Part 1- Conformity Scores\n",
    "\n",
    "Implementing the conformal predictor was challenging. At first I tried splitting the training set into three classes, one for each corresponding label in the iris dataset, but quickly realised this would make copmutation very long and confusing. First I decided to calcualte the conformity scores. This just required the training data and the labels, since we can simply augment the training data with the test samples before calling this method. The outer loop iterates over all samples in the training set, and the inner loop iterates over all remaining samples in the training set. The corresponding labels are then compared and if they are the same, we know they are of the same class, otherwise they are different. This was important since our conformity measure was the distance to nearest sample in a different class/distance to nearest sample in the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb91203c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.4 3.9 1.7 0.4]\n",
      "[1 0 2 1 2 0 0 2 2 1 0 0 1 1 0 2 1 2 0 0 1 1 2 1 2 2 1 2 0 0 0 2 1 1 2 0 2\n",
      " 0 2 2 0 0 2 0 0 1 2 1 1 1 2 2 2 1 0 1 1 1 0 2 2 1 0 0 2 2 1 1 1 1 1 0 2 0\n",
      " 1 0 2 0 2 0 0 1 1 0 1 0 2 1 0 0 1 0 0 1 0 2 0 2 0 1 0 2 0 1 0 2 1 2 2 1 2\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "def calculate_conformity_scores(X_train, y_train):\n",
    "    conformity_scores = []\n",
    "\n",
    "    for i, x_train in enumerate(X_train):\n",
    "        same_class_distances = []\n",
    "        diff_class_distances = []\n",
    "\n",
    "        for j, x_compare in enumerate(X_train):\n",
    "            if i != j:  # Don't compare the sample to itself\n",
    "                dist = euclidean_distance(x_train, x_compare)\n",
    "                if y_train[i] == y_train[j]:  # Same class\n",
    "                    same_class_distances.append(dist)\n",
    "                else:  # Different class\n",
    "                    diff_class_distances.append(dist)\n",
    "\n",
    "        # Calculate minimum nearest distances\n",
    "        nearest_same_class = min(same_class_distances)\n",
    "        nearest_diff_class = min(diff_class_distances)\n",
    "\n",
    "        # Compute the conformity score\n",
    "        if nearest_same_class > 0:\n",
    "            conformity_score = nearest_diff_class / nearest_same_class\n",
    "        else:\n",
    "            conformity_score = np.inf  # Handle division by zero case\n",
    "\n",
    "        conformity_scores.append(conformity_score)\n",
    "\n",
    "    return np.array(conformity_scores)\n",
    "print(X_iris_test[10])\n",
    "print(y_iris_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a743184",
   "metadata": {},
   "source": [
    "# Conformity Score and rank check\n",
    "\n",
    "In the following two cells I manually augmented the training set with different samples from the test set and then worked out all the conformity scores and printed them out. Since I was appending the test sample to the end, the last element was always the conformity score for the test sample. The training set had size 112 initially but became 113 with the augmentation as expected. To compute the rank, I used the method shown in the lab3 worksheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4bd0322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.26929552  9.57427108  1.03390789  1.94935887  3.06819053 14.89966443\n",
      "  6.51920241  4.84464532  5.09901951  3.31662479 15.06651917  6.74536878\n",
      "  0.97182532  2.56347978 10.81665383  4.72581563  2.17944947  3.19374388\n",
      "  6.96419414  7.18438478  2.67261242  2.15165741  2.35796522  3.70809924\n",
      "  2.88675135  3.41565026  4.2031734   3.36650165 15.13274595 14.45683229\n",
      "  5.464169    5.34679673  5.83095189  2.39791576  0.80659929  3.17441697\n",
      "  3.44963766 14.17744688  3.02884682  2.07019668  4.46196043  7.31436942\n",
      "  1.60727513 14.33527119 19.6977156   4.09267639  3.60555128  2.84604989\n",
      "  1.4596009   4.5         1.55456318  2.64575131  4.96655481  6.40312424\n",
      " 11.03026141  5.95818764  2.64575131  2.92326094 12.68857754  1.73205081\n",
      "  0.4662524   6.59545298 13.52774926  9.23038461  0.747545    5.35412613\n",
      "  4.4907312   1.96638416  2.29128785  5.09901951  4.6291005   6.46291194\n",
      "  2.7774603  13.3041347   4.38748219 14.93318452  1.10194633  4.5669621\n",
      "  2.96954236  8.6986589  20.49390153  9.8488578   9.94987437  7.110243\n",
      "  0.84983659 13.07669683  3.03315018  2.26077666  5.10655687  8.95358401\n",
      "  2.03540098  8.70344759 13.96424004  6.36396103 13.54621718  3.51188458\n",
      " 20.90454496  1.36277029  6.97137002  3.31662479  8.20060973  4.47213595\n",
      "  9.04157066  4.22295315 19.67231557  5.19615242  0.52704628  2.76887462\n",
      "  3.39116499  2.50998008  4.50924975  2.58843582 14.47411483]\n",
      "113\n"
     ]
    }
   ],
   "source": [
    "augmented_X = np.append(X_iris_train, [X_iris_test[3]], axis=0)\n",
    "new_label = 0\n",
    "augmented_y = np.append(y_iris_train, new_label)\n",
    "conf_scores = calculate_conformity_scores(augmented_X, augmented_y)\n",
    "print(conf_scores)\n",
    "print(len(conf_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fd5091d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for n in range(conf_scores.size):\n",
    "    if 14.47411483 >= conf_scores[n]:\n",
    "        count = count + 1\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f439d",
   "metadata": {},
   "source": [
    "# Conformal Predictor Implementation Part 2- Rank calculation\n",
    "Manually changing the label for every sample in the training set would have taken far too long and been very inefficient, especially since there are 114 p-values for the iris dataset. 38 samples in the test set and 3 possible labels for each. The outer loop iterates over every sample in the test set. The inner loop iterates from 0 to 2, since these are the only possible labels for the iris dataset and augments the training set with each test sample and a postulated label. It then calculates the conformity scores for all elements in the augmented training set and the rank of the test sample for the postulated label. Then it moves onto the next label. Once all labels have been used, the outer loop progresses to the next sample in the test set and repeats the process. The print statements present each test sample index with the postulated labels and their corresponding p-values.\n",
    "# Observation\n",
    "One thing I noticed was that for quite a few samples in the test set, despite having the correct label, the p-value was very low. I suspect this could be due to the fact that the iris dataset has some overlap with features. It looks as if we have a new family (or, otherwise, the test sample is an unusual representative of an old family). A similar thing was seen with the ionosphere dataset as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5bad4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sample Index   Assigned Label P-Value   \n",
      "==================================================\n",
      "0                   0              0.0088    \n",
      "0                   1              0.4779    \n",
      "0                   2              0.0088    \n",
      "1                   0              0.0088    \n",
      "1                   1              0.3982    \n",
      "1                   2              0.0088    \n",
      "2                   0              0.0088    \n",
      "2                   1              0.0088    \n",
      "2                   2              0.6372    \n",
      "3                   0              0.9292    \n",
      "3                   1              0.0088    \n",
      "3                   2              0.0088    \n",
      "4                   0              0.9115    \n",
      "4                   1              0.0088    \n",
      "4                   2              0.0088    \n",
      "5                   0              0.0088    \n",
      "5                   1              0.0177    \n",
      "5                   2              0.1681    \n",
      "6                   0              0.0088    \n",
      "6                   1              0.0088    \n",
      "6                   2              0.4513    \n",
      "7                   0              0.0088    \n",
      "7                   1              0.1770    \n",
      "7                   2              0.0088    \n",
      "8                   0              0.0088    \n",
      "8                   1              0.1770    \n",
      "8                   2              0.0088    \n",
      "9                   0              0.0088    \n",
      "9                   1              0.0088    \n",
      "9                   2              0.3451    \n",
      "10                  0              0.6460    \n",
      "10                  1              0.0088    \n",
      "10                  2              0.0088    \n",
      "11                  0              0.0088    \n",
      "11                  1              0.3451    \n",
      "11                  2              0.0088    \n",
      "12                  0              0.0177    \n",
      "12                  1              0.0177    \n",
      "12                  2              1.0000    \n",
      "13                  0              0.0088    \n",
      "13                  1              0.6372    \n",
      "13                  2              0.0088    \n",
      "14                  0              0.0088    \n",
      "14                  1              0.4425    \n",
      "14                  2              0.0088    \n",
      "15                  0              0.9027    \n",
      "15                  1              0.0088    \n",
      "15                  2              0.0088    \n",
      "16                  0              0.0088    \n",
      "16                  1              0.7434    \n",
      "16                  2              0.0088    \n",
      "17                  0              0.0088    \n",
      "17                  1              0.1327    \n",
      "17                  2              0.0265    \n",
      "18                  0              0.0088    \n",
      "18                  1              0.0088    \n",
      "18                  2              0.4779    \n",
      "19                  0              0.0088    \n",
      "19                  1              0.3274    \n",
      "19                  2              0.0088    \n",
      "20                  0              0.7611    \n",
      "20                  1              0.0088    \n",
      "20                  2              0.0088    \n",
      "21                  0              0.0088    \n",
      "21                  1              0.0265    \n",
      "21                  2              0.1062    \n",
      "22                  0              0.0088    \n",
      "22                  1              0.0442    \n",
      "22                  2              0.0796    \n",
      "23                  0              0.0088    \n",
      "23                  1              0.0885    \n",
      "23                  2              0.0354    \n",
      "24                  0              0.0088    \n",
      "24                  1              0.4602    \n",
      "24                  2              0.0088    \n",
      "25                  0              1.0000    \n",
      "25                  1              0.0088    \n",
      "25                  2              0.0088    \n",
      "26                  0              0.0088    \n",
      "26                  1              0.0088    \n",
      "26                  2              0.7788    \n",
      "27                  0              0.9558    \n",
      "27                  1              0.0088    \n",
      "27                  2              0.0088    \n",
      "28                  0              0.0088    \n",
      "28                  1              0.0088    \n",
      "28                  2              0.6372    \n",
      "29                  0              0.0088    \n",
      "29                  1              0.0265    \n",
      "29                  2              0.1327    \n",
      "30                  0              0.7522    \n",
      "30                  1              0.0088    \n",
      "30                  2              0.0088    \n",
      "31                  0              0.0088    \n",
      "31                  1              0.0354    \n",
      "31                  2              0.0973    \n",
      "32                  0              0.9558    \n",
      "32                  1              0.0088    \n",
      "32                  2              0.0088    \n",
      "33                  0              0.9292    \n",
      "33                  1              0.0088    \n",
      "33                  2              0.0088    \n",
      "34                  0              0.0088    \n",
      "34                  1              0.0088    \n",
      "34                  2              0.7434    \n",
      "35                  0              0.0088    \n",
      "35                  1              0.3009    \n",
      "35                  2              0.0088    \n",
      "36                  0              0.6549    \n",
      "36                  1              0.0088    \n",
      "36                  2              0.0088    \n",
      "37                  0              0.0088    \n",
      "37                  1              0.1062    \n",
      "37                  2              0.0265    \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_p_values_iris(X_train, y_train, X_test):\n",
    "    # List to store results\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over every sample in the test set\n",
    "    for test_index in range(X_test.shape[0]):\n",
    "        # Iterate over each possible label\n",
    "        for new_label in range(3): #because labels are 0, 1, and 2\n",
    "            # Create the augmented dataset\n",
    "            augmented_X = np.append(X_train, [X_test[test_index]], axis=0)\n",
    "            augmented_y = np.append(y_train, new_label)\n",
    "            \n",
    "            # Calculate conformity scores\n",
    "            conf_scores = calculate_conformity_scores(augmented_X, augmented_y)\n",
    "            sample_conformity_score = conf_scores[-1]  # Conformity score for the test sample\n",
    "            \n",
    "            # Calculate the rank and p-value\n",
    "            count = np.sum(sample_conformity_score >= conf_scores)\n",
    "            p_value = count / conf_scores.size  # p-value based on rank\n",
    "            \n",
    "            # Store the result\n",
    "            results.append((test_index, new_label, p_value))\n",
    "    \n",
    "    # Print results in a formatted table\n",
    "    print(f\"{'Test Sample Index':<20}{'Assigned Label':<15}{'P-Value':<10}\")\n",
    "    print(\"=\" * 50)\n",
    "    for test_index, new_label, p_value in results:\n",
    "        print(f\"{test_index:<20}{new_label:<15}{p_value:<10.4f}\")\n",
    "    \n",
    "    return results\n",
    "iris_results = calculate_p_values_iris(X_iris_train, y_iris_train, X_iris_test);#semi colon just so that the jupyter notebook doesnt return the entire results array again in the output cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc9c7250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.       0.       0.10135  0.10811  0.       0.       0.       0.\n",
      "  0.5473   0.82432  0.31081  1.       0.       0.       0.       0.\n",
      "  0.37162 -1.       0.33108 -1.       0.       0.       0.       0.\n",
      " -0.42568 -1.       1.      -1.       0.55405 -0.23649  0.       0.\n",
      "  0.       0.     ]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(X_ion_train[0])\n",
    "print(y_ion_test[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8602b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.3535039   3.1239572   1.05306686  1.60089926  5.4719808   1.33934789\n",
      "  1.56756792  6.57309437  1.04112152  0.73335741  0.97491524  3.23808741\n",
      "  1.49547028  5.48156191  0.84463145  4.38693576  1.27192332  3.62677751\n",
      "  5.66320976  2.09233346  1.06827153  1.42408054  6.15247926  1.09784014\n",
      "  2.07640678  1.24605642  0.91549079  2.50245173  5.69623425  1.03376356\n",
      "  3.50347084  1.86063347  1.07581715  1.13646146  1.04193525  6.25605378\n",
      "  1.02865877  5.50523436  1.04788968  1.10647298  2.49862157  2.29965943\n",
      "  1.02705097  3.81362869  1.36462875  1.01197354  1.36927023  1.73141591\n",
      "  3.09907651  1.00190621  1.93831449  5.46297526  2.11750095  3.04708553\n",
      "  2.88534212  1.11680421  3.49374241  0.88263185  1.18539136  0.67051516\n",
      "  0.99855881  1.42293802  3.49740256  2.69419464  0.77444314  2.16102575\n",
      "  1.07417312  5.00703284  1.87761573  2.01715801  3.62348126  4.4763277\n",
      "  1.55910876  1.90394474  4.6319441   4.89663689  1.72620517  1.08552656\n",
      "  0.99301086  5.72188928  1.3334873   5.85973626  2.22925681 10.75845868\n",
      "  1.19199357  5.37855437  4.75606249  2.13397915  1.63547457  1.08112454\n",
      "  1.08355888  1.06011311  1.79927523  6.28473382  4.87401166  8.21232438\n",
      "  2.41333551  3.83992998  2.37034421  2.51265601  0.96859463  8.4742355\n",
      "  1.02415411  1.0101147   7.51295215  0.99717862  1.46315488  0.99969508\n",
      "  1.20268808  1.26888613  1.07230342  1.5597601   1.62696548  1.49511719\n",
      "  0.84489136  2.4032857  10.73727455  1.11001166  1.40664367  1.11502555\n",
      "  1.57899201  1.04237244  7.0041695   2.97704462  0.97517528  1.10036893\n",
      "  1.68158646  1.06697406  1.1466253  13.20954378  1.04978048  1.30297559\n",
      "  1.13804063  2.37710051  1.72772177  0.95658068  1.88152829  2.03437434\n",
      "  1.16504038  0.86910138  0.92191264  3.73958394 12.67479828  9.1682286\n",
      "  4.14606373  1.64856323  0.9234055   1.02270806         inf  4.82729059\n",
      "  3.39658286  1.05221768  1.30121071  4.40258906  5.65145636  6.97595551\n",
      "  3.78436152  1.02250404  2.88726818  2.86865521  0.68075911  1.07438092\n",
      "  2.00695402  1.17485923  1.02438131  1.8716843   1.0143425   3.42443366\n",
      "  1.13682988  2.52493614  8.69803452  1.07375925  5.81862852  1.88239018\n",
      "  2.23930264  5.06984625  4.57735668  0.9557809   1.04539612  5.00539137\n",
      "  8.55603135  3.56963944  6.7525749   6.83138502  0.98111837  1.03145738\n",
      "  2.33482261  1.26872991  1.23670065  1.0656599   1.01582427 13.93421556\n",
      "  2.45298741  1.4528631   5.89639196  0.68359221  6.88167919  2.35851853\n",
      "  0.92229418  1.16494147  2.38566924  1.05102192  5.35841598  7.18122864\n",
      "  6.29682469  1.65854434  1.11167917  0.99203814  0.99143793  1.38480593\n",
      "  1.87400937  0.93423741  0.69594733  1.10976932  0.82369469  4.72920486\n",
      "  1.98223678         inf  6.51146885  1.0160838   1.05540989 13.69801297\n",
      "  0.91468458  1.21079272  6.34246932  0.67022277  2.89578121  1.05293949\n",
      "  2.77742077  1.14316735  1.02669038  1.10678642  5.16612946  2.67499186\n",
      "  0.66932078  5.01339241  5.61948632  1.40217293  1.50255307  6.2236394\n",
      "  1.33507028  2.21862721  0.85193607  3.63704033  1.08648233  1.15671956\n",
      "  0.96680396  1.47156985  2.77327153  1.05251761 10.77400582  7.09292039\n",
      "  1.02756447  1.04074983  2.6175985  10.83627876 10.43360269  0.96098551\n",
      "  3.95527446  4.91919399  1.08278786  1.69196801  3.38683636  3.26779199]\n",
      "264\n",
      "186\n"
     ]
    }
   ],
   "source": [
    "augmented_X_ion = np.append(X_ion_train, [X_ion_test[4]], axis=0)\n",
    "new_label = 1\n",
    "augmented_y_ion = np.append(y_ion_train, new_label)\n",
    "conf_scores_ion = calculate_conformity_scores(augmented_X_ion, augmented_y_ion)\n",
    "print(conf_scores_ion)\n",
    "print(len(conf_scores_ion))\n",
    "\n",
    "count = 0\n",
    "for n in range(conf_scores_ion.size):\n",
    "    if 3.26779199 >= conf_scores_ion[n]:\n",
    "        count = count + 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ba150c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sample Index   Assigned Label P-Value   \n",
      "==================================================\n",
      "0                   1              0.2955    \n",
      "0                   -1             0.0644    \n",
      "1                   1              0.8258    \n",
      "1                   -1             0.0038    \n",
      "2                   1              0.4205    \n",
      "2                   -1             0.0265    \n",
      "3                   1              0.4205    \n",
      "3                   -1             0.0303    \n",
      "4                   1              0.7045    \n",
      "4                   -1             0.0038    \n",
      "5                   1              0.2955    \n",
      "5                   -1             0.0568    \n",
      "6                   1              0.7614    \n",
      "6                   -1             0.0038    \n",
      "7                   1              0.9735    \n",
      "7                   -1             0.0038    \n",
      "8                   1              0.1326    \n",
      "8                   -1             0.1326    \n",
      "9                   1              0.5265    \n",
      "9                   -1             0.0038    \n",
      "10                  1              0.6098    \n",
      "10                  -1             0.0038    \n",
      "11                  1              0.0568    \n",
      "11                  -1             0.3030    \n",
      "12                  1              0.6288    \n",
      "12                  -1             0.0038    \n",
      "13                  1              0.1098    \n",
      "13                  -1             0.1553    \n",
      "14                  1              0.4735    \n",
      "14                  -1             0.0038    \n",
      "15                  1              0.8750    \n",
      "15                  -1             0.0038    \n",
      "16                  1              0.5758    \n",
      "16                  -1             0.0038    \n",
      "17                  1              0.0038    \n",
      "17                  -1             0.5227    \n",
      "18                  1              0.0492    \n",
      "18                  -1             0.3485    \n",
      "19                  1              0.7045    \n",
      "19                  -1             0.0038    \n",
      "20                  1              0.9318    \n",
      "20                  -1             0.0038    \n",
      "21                  1              0.4280    \n",
      "21                  -1             0.0227    \n",
      "22                  1              0.9773    \n",
      "22                  -1             0.0038    \n",
      "23                  1              0.0909    \n",
      "23                  -1             0.1932    \n",
      "24                  1              0.4470    \n",
      "24                  -1             0.0227    \n",
      "25                  1              0.1136    \n",
      "25                  -1             0.1439    \n",
      "26                  1              0.4394    \n",
      "26                  -1             0.0265    \n",
      "27                  1              0.7652    \n",
      "27                  -1             0.0038    \n",
      "28                  1              0.1629    \n",
      "28                  -1             0.1023    \n",
      "29                  1              0.8788    \n",
      "29                  -1             0.0038    \n",
      "30                  1              0.8182    \n",
      "30                  -1             0.0038    \n",
      "31                  1              0.7727    \n",
      "31                  -1             0.0038    \n",
      "32                  1              0.9735    \n",
      "32                  -1             0.0038    \n",
      "33                  1              0.6250    \n",
      "33                  -1             0.0038    \n",
      "34                  1              0.7576    \n",
      "34                  -1             0.0038    \n",
      "35                  1              0.0795    \n",
      "35                  -1             0.2311    \n",
      "36                  1              0.1174    \n",
      "36                  -1             0.1364    \n",
      "37                  1              0.1818    \n",
      "37                  -1             0.0985    \n",
      "38                  1              0.9015    \n",
      "38                  -1             0.0038    \n",
      "39                  1              0.7008    \n",
      "39                  -1             0.0038    \n",
      "40                  1              0.5038    \n",
      "40                  -1             0.0038    \n",
      "41                  1              0.0341    \n",
      "41                  -1             0.3864    \n",
      "42                  1              0.4583    \n",
      "42                  -1             0.0152    \n",
      "43                  1              0.5568    \n",
      "43                  -1             0.0038    \n",
      "44                  1              0.3674    \n",
      "44                  -1             0.0379    \n",
      "45                  1              0.9924    \n",
      "45                  -1             0.0038    \n",
      "46                  1              0.4697    \n",
      "46                  -1             0.0038    \n",
      "47                  1              0.7045    \n",
      "47                  -1             0.0038    \n",
      "48                  1              0.5530    \n",
      "48                  -1             0.0076    \n",
      "49                  1              0.0909    \n",
      "49                  -1             0.1932    \n",
      "50                  1              0.1136    \n",
      "50                  -1             0.1364    \n",
      "51                  1              0.0909    \n",
      "51                  -1             0.1932    \n",
      "52                  1              0.0038    \n",
      "52                  -1             0.5227    \n",
      "53                  1              0.7765    \n",
      "53                  -1             0.0038    \n",
      "54                  1              0.5038    \n",
      "54                  -1             0.0038    \n",
      "55                  1              0.5909    \n",
      "55                  -1             0.0038    \n",
      "56                  1              0.1098    \n",
      "56                  -1             0.1553    \n",
      "57                  1              0.0379    \n",
      "57                  -1             0.3674    \n",
      "58                  1              0.7045    \n",
      "58                  -1             0.0038    \n",
      "59                  1              0.9621    \n",
      "59                  -1             0.0038    \n",
      "60                  1              0.3561    \n",
      "60                  -1             0.0455    \n",
      "61                  1              0.4280    \n",
      "61                  -1             0.0265    \n",
      "62                  1              0.4583    \n",
      "62                  -1             0.0152    \n",
      "63                  1              0.1098    \n",
      "63                  -1             0.1402    \n",
      "64                  1              0.3788    \n",
      "64                  -1             0.0303    \n",
      "65                  1              0.5720    \n",
      "65                  -1             0.0038    \n",
      "66                  1              0.0341    \n",
      "66                  -1             0.3977    \n",
      "67                  1              0.6705    \n",
      "67                  -1             0.0038    \n",
      "68                  1              0.0530    \n",
      "68                  -1             0.3485    \n",
      "69                  1              0.5530    \n",
      "69                  -1             0.0038    \n",
      "70                  1              0.1061    \n",
      "70                  -1             0.1629    \n",
      "71                  1              0.3864    \n",
      "71                  -1             0.0341    \n",
      "72                  1              0.9545    \n",
      "72                  -1             0.0038    \n",
      "73                  1              0.7538    \n",
      "73                  -1             0.0038    \n",
      "74                  1              0.7008    \n",
      "74                  -1             0.0038    \n",
      "75                  1              0.9583    \n",
      "75                  -1             0.0038    \n",
      "76                  1              0.9508    \n",
      "76                  -1             0.0038    \n",
      "77                  1              0.0985    \n",
      "77                  -1             0.1780    \n",
      "78                  1              0.4697    \n",
      "78                  -1             0.0038    \n",
      "79                  1              0.0758    \n",
      "79                  -1             0.2765    \n",
      "80                  1              0.2424    \n",
      "80                  -1             0.0795    \n",
      "81                  1              0.4053    \n",
      "81                  -1             0.0303    \n",
      "82                  1              0.4848    \n",
      "82                  -1             0.0038    \n",
      "83                  1              0.7652    \n",
      "83                  -1             0.0038    \n",
      "84                  1              0.8485    \n",
      "84                  -1             0.0038    \n",
      "85                  1              0.3636    \n",
      "85                  -1             0.0379    \n",
      "86                  1              0.9848    \n",
      "86                  -1             0.0038    \n",
      "87                  1              0.4205    \n",
      "87                  -1             0.0303    \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_p_values_ion(X_train, y_train, X_test):\n",
    "    # List to store results\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over every sample in the test set\n",
    "    for test_index in range(X_test.shape[0]):\n",
    "        # Iterate over each possible label\n",
    "        for new_label in [1,-1]: #because labels are only 1 and -1\n",
    "            # Create the augmented dataset\n",
    "            augmented_X = np.append(X_train, [X_test[test_index]], axis=0)\n",
    "            augmented_y = np.append(y_train, new_label)\n",
    "            \n",
    "            # Calculate conformity scores\n",
    "            conf_scores = calculate_conformity_scores(augmented_X, augmented_y)\n",
    "            sample_conformity_score = conf_scores[-1]  # Conformity score for the test sample\n",
    "            \n",
    "            # Calculate the rank and p-value\n",
    "            count = np.sum(sample_conformity_score >= conf_scores)\n",
    "            p_value = count / conf_scores.size  # p-value based on rank\n",
    "            \n",
    "            # Store the result\n",
    "            results.append((test_index, new_label, p_value))\n",
    "    \n",
    "    # Print results in a formatted table\n",
    "    print(f\"{'Test Sample Index':<20}{'Assigned Label':<15}{'P-Value':<10}\")\n",
    "    print(\"=\" * 50)\n",
    "    for test_index, new_label, p_value in results:\n",
    "        print(f\"{test_index:<20}{new_label:<15}{p_value:<10.4f}\")\n",
    "    \n",
    "    return results\n",
    "ion_results = calculate_p_values_ion(X_ion_train, y_ion_train, X_ion_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a835f13a",
   "metadata": {},
   "source": [
    "# Average False p-values method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5c887dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_false_p_value(results, y_test):\n",
    "    false_p_values = []\n",
    "    for test_index, new_label, p_value in results:\n",
    "        true_label = y_test[test_index]  # Get the true label for the test sample\n",
    "        if new_label != true_label:  # If the new label is not the true label\n",
    "            false_p_values.append(p_value)  # Collect the p-value\n",
    "\n",
    "    # Calculate the average false p-value\n",
    "    return np.mean(false_p_values) if false_p_values else np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a4e95f",
   "metadata": {},
   "source": [
    "# Average False p-values (iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7d8270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE FALSE P-Value for IRIS DATASET: 0.012342803912435956\n"
     ]
    }
   ],
   "source": [
    "average_false_p = average_false_p_value(iris_results, y_iris_test)\n",
    "print(\"AVERAGE FALSE P-Value for IRIS DATASET:\", average_false_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9313a1f0",
   "metadata": {},
   "source": [
    "# Average False p-values (ionosphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2624df15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE FALSE P-Value for IONOSPHERE DATASET: 0.04709022038567494\n"
     ]
    }
   ],
   "source": [
    "average_false_p = average_false_p_value(ion_results, y_ion_test)\n",
    "print(\"AVERAGE FALSE P-Value for IONOSPHERE DATASET:\", average_false_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a19258c",
   "metadata": {},
   "source": [
    "# Closing comments\n",
    "\n",
    "## Efficiency of conformal predictor\n",
    "My implementation of the conformal predictor was far from optimal. While the calculation of conformity scores was relatively fast, the computation of the rank and p-values, particularly for the ionosphere dataset took a very long time This is because there are 34 features and computing the distance between two points means calculating the sum of the square differences for 68 values at a time. For datasets larger than ionosphere it would be completely infeasible, although I am unsure what the most optimal implementation would be."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
